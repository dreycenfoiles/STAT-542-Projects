{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6a76459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: Matrix\n",
      "\n",
      "Loaded glmnet 4.1-4\n",
      "\n",
      "Type 'citation(\"pROC\")' for a citation.\n",
      "\n",
      "\n",
      "Attaching package: 'pROC'\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    cov, smooth, var\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(text2vec)\n",
    "library(glmnet)\n",
    "library(slam)\n",
    "library(pROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6887ba33",
   "metadata": {},
   "source": [
    "We first load in the entire dataset to create an inital document term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e26ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all data and clean the html tags.\n",
    "train = read.table(\"alldata.tsv\",\n",
    "                   stringsAsFactors = FALSE,\n",
    "                   header = TRUE)\n",
    "train$review = gsub('<.*?>', ' ', train$review)\n",
    "\n",
    "# construct DT (DocumentTerm) matrix (maximum 4-grams).\n",
    "stop_words = c(\"i\", \"me\", \"my\", \"myself\", \n",
    "               \"we\", \"our\", \"ours\", \"ourselves\", \n",
    "               \"you\", \"your\", \"yours\", \n",
    "               \"their\", \"they\", \"his\", \"her\", \n",
    "               \"she\", \"he\", \"a\", \"an\", \"and\",\n",
    "               \"is\", \"was\", \"are\", \"were\", \n",
    "               \"him\", \"himself\", \"has\", \"have\", \n",
    "               \"it\", \"its\", \"the\", \"us\")\n",
    "it_train = itoken(train$review,\n",
    "                  preprocessor = tolower, \n",
    "                  tokenizer = word_tokenizer)\n",
    "tmp.vocab = create_vocabulary(it_train, \n",
    "                              stopwords = stop_words, \n",
    "                              ngram = c(1L,4L))\n",
    "tmp.vocab = prune_vocabulary(tmp.vocab, term_count_min = 10,\n",
    "                             doc_proportion_max = 0.5,\n",
    "                             doc_proportion_min = 0.001)\n",
    "dtm_train  = create_dtm(it_train, vocab_vectorizer(tmp.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2290e905",
   "metadata": {},
   "source": [
    "We then use the t-test method to trim our vocuabulary to 2K.\n",
    "\n",
    "(describe this method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ae2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim the vocabulary size to 2K using t-test\n",
    "v.size = dim(dtm_train)[2]\n",
    "ytrain = train$sentiment\n",
    "\n",
    "summ = matrix(0, nrow=v.size, ncol=4)\n",
    "summ[,1] = colapply_simple_triplet_matrix(\n",
    "  as.simple_triplet_matrix(dtm_train[ytrain==1, ]), mean)\n",
    "summ[,2] = colapply_simple_triplet_matrix(\n",
    "  as.simple_triplet_matrix(dtm_train[ytrain==1, ]), var)\n",
    "summ[,3] = colapply_simple_triplet_matrix(\n",
    "  as.simple_triplet_matrix(dtm_train[ytrain==0, ]), mean)\n",
    "summ[,4] = colapply_simple_triplet_matrix(\n",
    "  as.simple_triplet_matrix(dtm_train[ytrain==0, ]), var)\n",
    "\n",
    "n1 = sum(ytrain); \n",
    "n = length(ytrain)\n",
    "n0 = n - n1\n",
    "\n",
    "myp = (summ[,1] - summ[,3])/\n",
    "  sqrt(summ[,2]/n1 + summ[,4]/n0)\n",
    "\n",
    "words = colnames(dtm_train)\n",
    "id = order(abs(myp), decreasing=TRUE)[1:2000]\n",
    "#pos.list = words[id[myp[id]>0]]\n",
    "#neg.list = words[id[myp[id]<0]]\n",
    "words = words[id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f7b5d1",
   "metadata": {},
   "source": [
    "Create a new vectorizer based on the reduced vocabulary, and then use that to create a new DTM based on the reduced vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f4e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = vocab_vectorizer(create_vocabulary(words, \n",
    "                                                ngram = c(1L, 2L)))\n",
    "dtm_reduced = create_dtm(it_train, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce58d3b",
   "metadata": {},
   "source": [
    "Run Lasso with logistic regression again, and pick out the columns that give a vocab size of less than 1K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4abba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso (with logistic regression)\n",
    "set.seed(7568)\n",
    "tmpfit = glmnet(x = dtm_reduced, \n",
    "                y = train$sentiment, \n",
    "                alpha = 0.05,\n",
    "                family='binomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dad87f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the largest df among the beta values thar are less than 1K \n",
    "# and store the corresponding words in myvocab\n",
    "i = 1\n",
    "while (tmpfit$df[i] <= 1000) {\n",
    "    i = i+1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7063750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "myvocab = colnames(dtm_reduced)[which(tmpfit$beta[, i-1] != 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165e6aad",
   "metadata": {},
   "source": [
    "Let's check this vocab against the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd94e9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = vocab_vectorizer(create_vocabulary(myvocab, \n",
    "                                                  ngram = c(1L, 2L)))\n",
    "dtm_test = create_dtm(it_train, vectorizer)\n",
    "\n",
    "testfit = glmnet(x = dtm_test, \n",
    "                 y = train$sentiment, \n",
    "                 alpha = 0.05,\n",
    "                 family='binomial')\n",
    "\n",
    "pred = predict(testfit, newx=dtm_test, s=0.01)\n",
    "\n",
    "roc_obj <- roc(train$sentiment, c(pred))\n",
    "pROC::auc(roc_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a890bb80",
   "metadata": {},
   "source": [
    "Just to be safe, let's also test the splits. First we create them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9892f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- read.table(\"alldata.tsv\", stringsAsFactors = FALSE,\n",
    "                  header = TRUE)\n",
    "testIDs <- read.csv(\"project3_splits.csv\", header = TRUE)\n",
    "for(j in 1:5){\n",
    "  dir.create(paste(\"split_\", j, sep=\"\"))\n",
    "  train <- data[-testIDs[,j], c(\"id\", \"sentiment\", \"review\") ]\n",
    "  test <- data[testIDs[,j], c(\"id\", \"review\")]\n",
    "  test.y <- data[testIDs[,j], c(\"id\", \"sentiment\", \"score\")]\n",
    "  \n",
    "  tmp_file_name <- paste(\"split_\", j, \"/\", \"train.tsv\", sep=\"\")\n",
    "  write.table(train, file=tmp_file_name, \n",
    "              quote=TRUE, \n",
    "              row.names = FALSE,\n",
    "              sep='\\t')\n",
    "  tmp_file_name <- paste(\"split_\", j, \"/\", \"test.tsv\", sep=\"\")\n",
    "  write.table(test, file=tmp_file_name, \n",
    "              quote=TRUE, \n",
    "              row.names = FALSE,\n",
    "              sep='\\t')\n",
    "  tmp_file_name <- paste(\"split_\", j, \"/\", \"test_y.tsv\", sep=\"\")\n",
    "  write.table(test.y, file=tmp_file_name, \n",
    "            quote=TRUE, \n",
    "            row.names = FALSE,\n",
    "            sep='\\t')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7c7ed4",
   "metadata": {},
   "source": [
    "And now we test our vocab on each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe288327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this loop is for our purposes, we will not submit it\n",
    "scores = {}\n",
    "for (j in 1:5) {\n",
    "    setwd(paste(\"split_\", j, sep=\"\"))\n",
    "    # below this comment is submitted code unless otherwise stated\n",
    "    train = read.table(\"train.tsv\",\n",
    "                       stringsAsFactors = FALSE,\n",
    "                       header = TRUE)\n",
    "    train$review <- gsub('<.*?>', ' ', train$review)\n",
    "\n",
    "    # Create matrix corresponding to vocab\n",
    "    it_train = itoken(train$review,\n",
    "                        preprocessor = tolower, \n",
    "                        tokenizer = word_tokenizer)\n",
    "    vectorizer = vocab_vectorizer(create_vocabulary(myvocab, \n",
    "                                                      ngram = c(1L, 2L)))\n",
    "    dtm_train = create_dtm(it_train, vectorizer)\n",
    "    set.seed(7568)\n",
    "    fit = glmnet(x = dtm_train, \n",
    "                    y = train$sentiment, \n",
    "                    alpha = 0.05,\n",
    "                    family='binomial')\n",
    "    \n",
    "    #####################################\n",
    "    # Load test data, and \n",
    "    # Compute prediction\n",
    "    #####################################\n",
    "    test <- read.table(\"test.tsv\", stringsAsFactors = FALSE,\n",
    "                    header = TRUE)\n",
    "\n",
    "    test$review <- gsub('<.*?>', ' ', test$review)\n",
    "\n",
    "    # Create matrix corresponding to vocab\n",
    "    it_test = itoken(test$review,\n",
    "                        preprocessor = tolower, \n",
    "                        tokenizer = word_tokenizer)\n",
    "    vectorizer = vocab_vectorizer(create_vocabulary(myvocab, \n",
    "                                                ngram = c(1L, 2L)))\n",
    "    dtm_test = create_dtm(it_test, vectorizer)\n",
    "    \n",
    "    prob = predict(fit, newx=dtm_test, s=0.05)\n",
    "    output = data.frame(id=c(test$id),\n",
    "                        prob=c(prob))\n",
    "    #####################################\n",
    "    # Store your prediction for test data in a data frame\n",
    "    # \"output\": col 1 is test$id\n",
    "    #           col 2 is the predicted probs\n",
    "    #####################################\n",
    "    write.table(output, file = \"mysubmission.txt\", \n",
    "                row.names = FALSE, sep='\\t')\n",
    "    \n",
    "    # Below this is not submitted code\n",
    "    # move \"test_y.tsv\" to this directory\n",
    "    test.y <- read.table(\"test_y.tsv\", header = TRUE)\n",
    "    pred <- read.table(\"mysubmission.txt\", header = TRUE)\n",
    "    pred <- merge(pred, test.y, by=\"id\")\n",
    "    roc_obj <- roc(pred$sentiment, pred$prob)\n",
    "    score = pROC::auc(roc_obj)\n",
    "    scores[j] = score\n",
    "    setwd('../')\n",
    "    }\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d0f700",
   "metadata": {},
   "source": [
    "It looks good, so we write it to our file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5fea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write vocab to text file\n",
    "some.strs <- c(myvocab)\n",
    "write.table(some.strs, file = \"myvocab.txt\",\n",
    "            quote = FALSE,\n",
    "            row.names = FALSE,\n",
    "            col.names = FALSE,\n",
    "            sep = \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "2332779e627334dcd37f25e3f805971bc5906ed7e754eda45dfa1caa829d1c25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
