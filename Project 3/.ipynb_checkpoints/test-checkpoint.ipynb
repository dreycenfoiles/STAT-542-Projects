{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c0d1d4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "library(text2vec)\n",
    "library(glmnet)\n",
    "library(slam)\n",
    "library(pROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d482de4",
   "metadata": {},
   "source": [
    "### Split generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecb4a143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in dir.create(paste(\"split_\", j, sep = \"\")):\n",
      "\"'split_1' already exists\"\n",
      "Warning message in dir.create(paste(\"split_\", j, sep = \"\")):\n",
      "\"'split_2' already exists\"\n",
      "Warning message in dir.create(paste(\"split_\", j, sep = \"\")):\n",
      "\"'split_3' already exists\"\n",
      "Warning message in dir.create(paste(\"split_\", j, sep = \"\")):\n",
      "\"'split_4' already exists\"\n",
      "Warning message in dir.create(paste(\"split_\", j, sep = \"\")):\n",
      "\"'split_5' already exists\"\n"
     ]
    }
   ],
   "source": [
    "data <- read.table(\"alldata.tsv\", stringsAsFactors = FALSE,\n",
    "                  header = TRUE)\n",
    "testIDs <- read.csv(\"project3_splits.csv\", header = TRUE)\n",
    "for(j in 1:5){\n",
    "  dir.create(paste(\"split_\", j, sep=\"\"))\n",
    "  train <- data[-testIDs[,j], c(\"id\", \"sentiment\", \"review\") ]\n",
    "  test <- data[testIDs[,j], c(\"id\", \"review\")]\n",
    "  test.y <- data[testIDs[,j], c(\"id\", \"sentiment\", \"score\")]\n",
    "  \n",
    "  tmp_file_name <- paste(\"split_\", j, \"/\", \"train.tsv\", sep=\"\")\n",
    "  write.table(train, file=tmp_file_name, \n",
    "              quote=TRUE, \n",
    "              row.names = FALSE,\n",
    "              sep='\\t')\n",
    "  tmp_file_name <- paste(\"split_\", j, \"/\", \"test.tsv\", sep=\"\")\n",
    "  write.table(test, file=tmp_file_name, \n",
    "              quote=TRUE, \n",
    "              row.names = FALSE,\n",
    "              sep='\\t')\n",
    "  tmp_file_name <- paste(\"split_\", j, \"/\", \"test_y.tsv\", sep=\"\")\n",
    "  write.table(test.y, file=tmp_file_name, \n",
    "            quote=TRUE, \n",
    "            row.names = FALSE,\n",
    "            sep='\\t')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273bdab0",
   "metadata": {},
   "source": [
    "### Generate Initial document term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8322305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all data and clean the html tags.\n",
    "train = read.table(\"alldata.tsv\",\n",
    "                   stringsAsFactors = FALSE,\n",
    "                   header = TRUE)\n",
    "train$review = gsub('<.*?>', ' ', train$review)\n",
    "\n",
    "# construct DT (DocumentTerm) matrix (maximum 4-grams).\n",
    "stop_words = c(\"i\", \"me\", \"my\", \"myself\", \n",
    "               \"we\", \"our\", \"ours\", \"ourselves\", \n",
    "               \"you\", \"your\", \"yours\", \n",
    "               \"their\", \"they\", \"his\", \"her\", \n",
    "               \"she\", \"he\", \"a\", \"an\", \"and\",\n",
    "               \"is\", \"was\", \"are\", \"were\", \n",
    "               \"him\", \"himself\", \"has\", \"have\", \n",
    "               \"it\", \"its\", \"the\", \"us\")\n",
    "it_train = itoken(train$review,\n",
    "                  preprocessor = tolower, \n",
    "                  tokenizer = word_tokenizer)\n",
    "tmp.vocab = create_vocabulary(it_train, \n",
    "                              stopwords = stop_words, \n",
    "                              ngram = c(1L,4L))\n",
    "tmp.vocab = prune_vocabulary(tmp.vocab, term_count_min = 10,\n",
    "                             doc_proportion_max = 0.5,\n",
    "                             doc_proportion_min = 0.001)\n",
    "dtm_train  = create_dtm(it_train, vocab_vectorizer(tmp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0c5f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim the vocabulary size to 2K using t-test\n",
    "v.size = dim(dtm_train)[2]\n",
    "ytrain = train$sentiment\n",
    "\n",
    "summ = matrix(0, nrow=v.size, ncol=4)\n",
    "summ[,1] = colapply_simple_triplet_matrix(\n",
    "  as.simple_triplet_matrix(dtm_train[ytrain==1, ]), mean)\n",
    "summ[,2] = colapply_simple_triplet_matrix(\n",
    "  as.simple_triplet_matrix(dtm_train[ytrain==1, ]), var)\n",
    "summ[,3] = colapply_simple_triplet_matrix(\n",
    "  as.simple_triplet_matrix(dtm_train[ytrain==0, ]), mean)\n",
    "summ[,4] = colapply_simple_triplet_matrix(\n",
    "  as.simple_triplet_matrix(dtm_train[ytrain==0, ]), var)\n",
    "\n",
    "n1 = sum(ytrain); \n",
    "n = length(ytrain)\n",
    "n0 = n - n1\n",
    "\n",
    "myp = (summ[,1] - summ[,3])/\n",
    "  sqrt(summ[,2]/n1 + summ[,4]/n0)\n",
    "\n",
    "words = colnames(dtm_train)\n",
    "id = order(abs(myp), decreasing=TRUE)[1:2000]\n",
    "pos.list = words[id[myp[id]>0]]\n",
    "neg.list = words[id[myp[id]<0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd8f71e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd48965",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = vocab_vectorizer(create_vocabulary(words, \n",
    "                                                ngram = c(1L, 2L)))\n",
    "dtm_reduced = create_dtm(it_train, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1503e9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Lasso (with logistic regression) to trim the vocabulary size to 2K.\n",
    "set.seed(7568)\n",
    "tmpfit = glmnet(x = dtm_reduced, \n",
    "                y = train$sentiment, \n",
    "                alpha = 0.05,\n",
    "                family='binomial')\n",
    "tmpfit$df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67dc390",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpfit$df[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd71058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the largest df among the beta values thar are less than 1K \n",
    "#(which turns out to be the 31st column), and store the corresponding words in myvocab\n",
    "myvocab = colnames(dtm_reduced)[which(tmpfit$beta[, 31] != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5481e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = vocab_vectorizer(create_vocabulary(myvocab, \n",
    "                                                  ngram = c(1L, 2L)))\n",
    "dtm_test = create_dtm(it_train, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d6b64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "testfit = glmnet(x = dtm_test, \n",
    "                 y = train$sentiment, \n",
    "                 alpha = 0.05,\n",
    "                 family='binomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18e1764",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict(testfit, newx=dtm_test, s=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a170ffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_obj <- roc(train$sentiment, c(pred))\n",
    "pROC::auc(roc_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d809441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write vocab to text file\n",
    "some.strs <- c(myvocab)\n",
    "write.table(some.strs, file = \"myvocab.txt\",\n",
    "            quote = FALSE,\n",
    "            row.names = FALSE,\n",
    "            col.names = FALSE,\n",
    "            sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aac4e1",
   "metadata": {},
   "source": [
    "### Submitted code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ede573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is appended to the top of the submitted code\n",
    "library(text2vec)\n",
    "library(glmnet)\n",
    "myvocab <- scan(file = \"myvocab.txt\", what = character())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10a5d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this loop is for our purposes, we will not submit it\n",
    "scores = {}\n",
    "for (j in 1:5) {\n",
    "    setwd(paste(\"split_\", j, sep=\"\"))\n",
    "    # below this comment is submitted code unless otherwise stated\n",
    "    train = read.table(\"train.tsv\",\n",
    "                       stringsAsFactors = FALSE,\n",
    "                       header = TRUE)\n",
    "    train$review <- gsub('<.*?>', ' ', train$review)\n",
    "\n",
    "    # Create matrix corresponding to vocab\n",
    "    it_train = itoken(train$review,\n",
    "                        preprocessor = tolower, \n",
    "                        tokenizer = word_tokenizer)\n",
    "    vectorizer = vocab_vectorizer(create_vocabulary(myvocab, \n",
    "                                                      ngram = c(1L, 2L)))\n",
    "    dtm_train = create_dtm(it_train, vectorizer)\n",
    "    set.seed(7568)\n",
    "    fit = glmnet(x = dtm_train, \n",
    "                    y = train$sentiment, \n",
    "                    alpha = 0.05,\n",
    "                    family='binomial')\n",
    "    \n",
    "    #####################################\n",
    "    # Load test data, and \n",
    "    # Compute prediction\n",
    "    #####################################\n",
    "    test <- read.table(\"test.tsv\", stringsAsFactors = FALSE,\n",
    "                    header = TRUE)\n",
    "\n",
    "    test$review <- gsub('<.*?>', ' ', test$review)\n",
    "\n",
    "    # Create matrix corresponding to vocab\n",
    "    it_test = itoken(test$review,\n",
    "                        preprocessor = tolower, \n",
    "                        tokenizer = word_tokenizer)\n",
    "    vectorizer = vocab_vectorizer(create_vocabulary(myvocab, \n",
    "                                                ngram = c(1L, 2L)))\n",
    "    dtm_test = create_dtm(it_test, vectorizer)\n",
    "    \n",
    "    prob = predict(fit, newx=dtm_test, s=0.05)\n",
    "    output = data.frame(id=c(test$id),\n",
    "                        prob=c(prob))\n",
    "    #####################################\n",
    "    # Store your prediction for test data in a data frame\n",
    "    # \"output\": col 1 is test$id\n",
    "    #           col 2 is the predicted probs\n",
    "    #####################################\n",
    "    write.table(output, file = \"mysubmission.txt\", \n",
    "                row.names = FALSE, sep='\\t')\n",
    "    \n",
    "    # Below this is not submitted code\n",
    "    # move \"test_y.tsv\" to this directory\n",
    "    test.y <- read.table(\"test_y.tsv\", header = TRUE)\n",
    "    pred <- read.table(\"mysubmission.txt\", header = TRUE)\n",
    "    pred <- merge(pred, test.y, by=\"id\")\n",
    "    roc_obj <- roc(pred$sentiment, pred$prob)\n",
    "    score = pROC::auc(roc_obj)\n",
    "    scores[j] = score\n",
    "    setwd('../')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf13e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac43566",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
