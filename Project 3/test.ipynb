{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c0d1d4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: Matrix\n",
      "\n",
      "Loaded glmnet 4.1-4\n",
      "\n",
      "Type 'citation(\"pROC\")' for a citation.\n",
      "\n",
      "\n",
      "Attaching package: ‘pROC’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    cov, smooth, var\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(text2vec)\n",
    "library(glmnet)\n",
    "library(slam)\n",
    "library(pROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d482de4",
   "metadata": {},
   "source": [
    "### Split generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecb4a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- read.table(\"alldata.tsv\", stringsAsFactors = FALSE,\n",
    "                  header = TRUE)\n",
    "testIDs <- read.csv(\"project3_splits.csv\", header = TRUE)\n",
    "for(j in 1:5){\n",
    "  dir.create(paste(\"split_\", j, sep=\"\"))\n",
    "  train <- data[-testIDs[,j], c(\"id\", \"sentiment\", \"review\") ]\n",
    "  test <- data[testIDs[,j], c(\"id\", \"review\")]\n",
    "  test.y <- data[testIDs[,j], c(\"id\", \"sentiment\", \"score\")]\n",
    "  \n",
    "  tmp_file_name <- paste(\"split_\", j, \"/\", \"train.tsv\", sep=\"\")\n",
    "  write.table(train, file=tmp_file_name, \n",
    "              quote=TRUE, \n",
    "              row.names = FALSE,\n",
    "              sep='\\t')\n",
    "  tmp_file_name <- paste(\"split_\", j, \"/\", \"test.tsv\", sep=\"\")\n",
    "  write.table(test, file=tmp_file_name, \n",
    "              quote=TRUE, \n",
    "              row.names = FALSE,\n",
    "              sep='\\t')\n",
    "  tmp_file_name <- paste(\"split_\", j, \"/\", \"test_y.tsv\", sep=\"\")\n",
    "  write.table(test.y, file=tmp_file_name, \n",
    "            quote=TRUE, \n",
    "            row.names = FALSE,\n",
    "            sep='\\t')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273bdab0",
   "metadata": {},
   "source": [
    "### Generate Initial document term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8322305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all data and clean the html tags.\n",
    "train = read.table(\"alldata.tsv\",\n",
    "                   stringsAsFactors = FALSE,\n",
    "                   header = TRUE)\n",
    "train$review = gsub('<.*?>', ' ', train$review)\n",
    "\n",
    "# construct DT (DocumentTerm) matrix (maximum 4-grams).\n",
    "stop_words = c(\"i\", \"me\", \"my\", \"myself\", \n",
    "               \"we\", \"our\", \"ours\", \"ourselves\", \n",
    "               \"you\", \"your\", \"yours\", \n",
    "               \"their\", \"they\", \"his\", \"her\", \n",
    "               \"she\", \"he\", \"a\", \"an\", \"and\",\n",
    "               \"is\", \"was\", \"are\", \"were\", \n",
    "               \"him\", \"himself\", \"has\", \"have\", \n",
    "               \"it\", \"its\", \"the\", \"us\")\n",
    "it_train = itoken(train$review,\n",
    "                  preprocessor = tolower, \n",
    "                  tokenizer = word_tokenizer)\n",
    "tmp.vocab = create_vocabulary(it_train, \n",
    "                              stopwords = stop_words, \n",
    "                              ngram = c(1L,4L))\n",
    "tmp.vocab = prune_vocabulary(tmp.vocab, term_count_min = 10,\n",
    "                             doc_proportion_max = 0.5,\n",
    "                             doc_proportion_min = 0.001)\n",
    "dtm_train  = create_dtm(it_train, vocab_vectorizer(tmp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb0c5f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim the vocabulary size to 2K using t-test\n",
    "v.size = dim(dtm_train)[2]\n",
    "ytrain = train$sentiment\n",
    "\n",
    "summ = matrix(0, nrow=v.size, ncol=4)\n",
    "summ[,1] = colapply_simple_triplet_matrix(\n",
    "  as.simple_triplet_matrix(dtm_train[ytrain==1, ]), mean)\n",
    "summ[,2] = colapply_simple_triplet_matrix(\n",
    "  as.simple_triplet_matrix(dtm_train[ytrain==1, ]), var)\n",
    "summ[,3] = colapply_simple_triplet_matrix(\n",
    "  as.simple_triplet_matrix(dtm_train[ytrain==0, ]), mean)\n",
    "summ[,4] = colapply_simple_triplet_matrix(\n",
    "  as.simple_triplet_matrix(dtm_train[ytrain==0, ]), var)\n",
    "\n",
    "n1 = sum(ytrain); \n",
    "n = length(ytrain)\n",
    "n0 = n - n1\n",
    "\n",
    "myp = (summ[,1] - summ[,3])/\n",
    "  sqrt(summ[,2]/n1 + summ[,4]/n0)\n",
    "\n",
    "words = colnames(dtm_train)\n",
    "id = order(abs(myp), decreasing=TRUE)[1:2000]\n",
    "pos.list = words[id[myp[id]>0]]\n",
    "neg.list = words[id[myp[id]<0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "afd8f71e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): object 'words' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): object 'words' not found\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "words[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebd48965",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = vocab_vectorizer(create_vocabulary(words, \n",
    "                                                ngram = c(1L, 2L)))\n",
    "dtm_reduced = create_dtm(it_train, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1503e9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>0</li><li>1</li><li>2</li><li>3</li><li>4</li><li>5</li><li>7</li><li>14</li><li>18</li><li>26</li><li>38</li><li>49</li><li>60</li><li>75</li><li>100</li><li>126</li><li>152</li><li>177</li><li>208</li><li>242</li><li>288</li><li>330</li><li>371</li><li>419</li><li>470</li><li>528</li><li>584</li><li>674</li><li>754</li><li>855</li><li>935</li><li>1052</li><li>1187</li><li>1321</li><li>1472</li><li>1645</li><li>1838</li><li>2076</li><li>2334</li><li>2612</li><li>2891</li><li>3238</li><li>3583</li><li>3951</li><li>4336</li><li>4779</li><li>5259</li><li>5727</li><li>6186</li><li>6643</li><li>7099</li><li>7612</li><li>8112</li><li>8611</li><li>9082</li><li>9564</li><li>10027</li><li>10477</li><li>10908</li><li>11343</li><li>11763</li><li>12196</li><li>12600</li><li>12945</li><li>13316</li><li>13662</li><li>14041</li><li>14359</li><li>14686</li><li>14984</li><li>15262</li><li>15532</li><li>15780</li><li>16032</li><li>16252</li><li>16466</li><li>16686</li><li>16887</li><li>17057</li><li>17222</li><li>17396</li><li>17546</li><li>17683</li><li>17832</li><li>17978</li><li>18111</li><li>18235</li><li>18352</li><li>18506</li><li>18628</li><li>18726</li><li>18842</li><li>18942</li><li>19052</li><li>19158</li><li>19256</li><li>19347</li><li>19434</li><li>19519</li><li>19601</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 3\n",
       "\\item 4\n",
       "\\item 5\n",
       "\\item 7\n",
       "\\item 14\n",
       "\\item 18\n",
       "\\item 26\n",
       "\\item 38\n",
       "\\item 49\n",
       "\\item 60\n",
       "\\item 75\n",
       "\\item 100\n",
       "\\item 126\n",
       "\\item 152\n",
       "\\item 177\n",
       "\\item 208\n",
       "\\item 242\n",
       "\\item 288\n",
       "\\item 330\n",
       "\\item 371\n",
       "\\item 419\n",
       "\\item 470\n",
       "\\item 528\n",
       "\\item 584\n",
       "\\item 674\n",
       "\\item 754\n",
       "\\item 855\n",
       "\\item 935\n",
       "\\item 1052\n",
       "\\item 1187\n",
       "\\item 1321\n",
       "\\item 1472\n",
       "\\item 1645\n",
       "\\item 1838\n",
       "\\item 2076\n",
       "\\item 2334\n",
       "\\item 2612\n",
       "\\item 2891\n",
       "\\item 3238\n",
       "\\item 3583\n",
       "\\item 3951\n",
       "\\item 4336\n",
       "\\item 4779\n",
       "\\item 5259\n",
       "\\item 5727\n",
       "\\item 6186\n",
       "\\item 6643\n",
       "\\item 7099\n",
       "\\item 7612\n",
       "\\item 8112\n",
       "\\item 8611\n",
       "\\item 9082\n",
       "\\item 9564\n",
       "\\item 10027\n",
       "\\item 10477\n",
       "\\item 10908\n",
       "\\item 11343\n",
       "\\item 11763\n",
       "\\item 12196\n",
       "\\item 12600\n",
       "\\item 12945\n",
       "\\item 13316\n",
       "\\item 13662\n",
       "\\item 14041\n",
       "\\item 14359\n",
       "\\item 14686\n",
       "\\item 14984\n",
       "\\item 15262\n",
       "\\item 15532\n",
       "\\item 15780\n",
       "\\item 16032\n",
       "\\item 16252\n",
       "\\item 16466\n",
       "\\item 16686\n",
       "\\item 16887\n",
       "\\item 17057\n",
       "\\item 17222\n",
       "\\item 17396\n",
       "\\item 17546\n",
       "\\item 17683\n",
       "\\item 17832\n",
       "\\item 17978\n",
       "\\item 18111\n",
       "\\item 18235\n",
       "\\item 18352\n",
       "\\item 18506\n",
       "\\item 18628\n",
       "\\item 18726\n",
       "\\item 18842\n",
       "\\item 18942\n",
       "\\item 19052\n",
       "\\item 19158\n",
       "\\item 19256\n",
       "\\item 19347\n",
       "\\item 19434\n",
       "\\item 19519\n",
       "\\item 19601\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0\n",
       "2. 1\n",
       "3. 2\n",
       "4. 3\n",
       "5. 4\n",
       "6. 5\n",
       "7. 7\n",
       "8. 14\n",
       "9. 18\n",
       "10. 26\n",
       "11. 38\n",
       "12. 49\n",
       "13. 60\n",
       "14. 75\n",
       "15. 100\n",
       "16. 126\n",
       "17. 152\n",
       "18. 177\n",
       "19. 208\n",
       "20. 242\n",
       "21. 288\n",
       "22. 330\n",
       "23. 371\n",
       "24. 419\n",
       "25. 470\n",
       "26. 528\n",
       "27. 584\n",
       "28. 674\n",
       "29. 754\n",
       "30. 855\n",
       "31. 935\n",
       "32. 1052\n",
       "33. 1187\n",
       "34. 1321\n",
       "35. 1472\n",
       "36. 1645\n",
       "37. 1838\n",
       "38. 2076\n",
       "39. 2334\n",
       "40. 2612\n",
       "41. 2891\n",
       "42. 3238\n",
       "43. 3583\n",
       "44. 3951\n",
       "45. 4336\n",
       "46. 4779\n",
       "47. 5259\n",
       "48. 5727\n",
       "49. 6186\n",
       "50. 6643\n",
       "51. 7099\n",
       "52. 7612\n",
       "53. 8112\n",
       "54. 8611\n",
       "55. 9082\n",
       "56. 9564\n",
       "57. 10027\n",
       "58. 10477\n",
       "59. 10908\n",
       "60. 11343\n",
       "61. 11763\n",
       "62. 12196\n",
       "63. 12600\n",
       "64. 12945\n",
       "65. 13316\n",
       "66. 13662\n",
       "67. 14041\n",
       "68. 14359\n",
       "69. 14686\n",
       "70. 14984\n",
       "71. 15262\n",
       "72. 15532\n",
       "73. 15780\n",
       "74. 16032\n",
       "75. 16252\n",
       "76. 16466\n",
       "77. 16686\n",
       "78. 16887\n",
       "79. 17057\n",
       "80. 17222\n",
       "81. 17396\n",
       "82. 17546\n",
       "83. 17683\n",
       "84. 17832\n",
       "85. 17978\n",
       "86. 18111\n",
       "87. 18235\n",
       "88. 18352\n",
       "89. 18506\n",
       "90. 18628\n",
       "91. 18726\n",
       "92. 18842\n",
       "93. 18942\n",
       "94. 19052\n",
       "95. 19158\n",
       "96. 19256\n",
       "97. 19347\n",
       "98. 19434\n",
       "99. 19519\n",
       "100. 19601\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  [1]     0     1     2     3     4     5     7    14    18    26    38    49\n",
       " [13]    60    75   100   126   152   177   208   242   288   330   371   419\n",
       " [25]   470   528   584   674   754   855   935  1052  1187  1321  1472  1645\n",
       " [37]  1838  2076  2334  2612  2891  3238  3583  3951  4336  4779  5259  5727\n",
       " [49]  6186  6643  7099  7612  8112  8611  9082  9564 10027 10477 10908 11343\n",
       " [61] 11763 12196 12600 12945 13316 13662 14041 14359 14686 14984 15262 15532\n",
       " [73] 15780 16032 16252 16466 16686 16887 17057 17222 17396 17546 17683 17832\n",
       " [85] 17978 18111 18235 18352 18506 18628 18726 18842 18942 19052 19158 19256\n",
       " [97] 19347 19434 19519 19601"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use Lasso (with logistic regression) to trim the vocabulary size to 2K.\n",
    "set.seed(7568)\n",
    "tmpfit = glmnet(x = dtm_reduced, \n",
    "                y = train$sentiment, \n",
    "                alpha = 0.05,\n",
    "                family='binomial')\n",
    "tmpfit$df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d67dc390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "935"
      ],
      "text/latex": [
       "935"
      ],
      "text/markdown": [
       "935"
      ],
      "text/plain": [
       "[1] 935"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmpfit$df[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd71058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the largest df among the beta values thar are less than 1K \n",
    "#(which turns out to be the 31st column), and store the corresponding words in myvocab\n",
    "myvocab = colnames(dtm_reduced)[which(tmpfit$beta[, 31] != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5481e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = vocab_vectorizer(create_vocabulary(myvocab, \n",
    "                                                  ngram = c(1L, 2L)))\n",
    "dtm_test = create_dtm(it_train, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7d6b64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "testfit = glmnet(x = dtm_test, \n",
    "                 y = train$sentiment, \n",
    "                 alpha = 0.05,\n",
    "                 family='binomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b18e1764",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict(testfit, newx=dtm_test, s=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a170ffea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting levels: control = 0, case = 1\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.9715458464"
      ],
      "text/latex": [
       "0.9715458464"
      ],
      "text/markdown": [
       "0.9715458464"
      ],
      "text/plain": [
       "Area under the curve: 0.9715"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roc_obj <- roc(train$sentiment, c(pred))\n",
    "pROC::auc(roc_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d809441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write vocab to text file\n",
    "some.strs <- c(myvocab)\n",
    "write.table(some.strs, file = \"myvocab.txt\",\n",
    "            quote = FALSE,\n",
    "            row.names = FALSE,\n",
    "            col.names = FALSE,\n",
    "            sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aac4e1",
   "metadata": {},
   "source": [
    "### Submitted code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08ede573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is appended to the top of the submitted code\n",
    "library(text2vec)\n",
    "library(glmnet)\n",
    "myvocab <- scan(file = \"myvocab.txt\", what = character())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d10a5d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting levels: control = 0, case = 1\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting levels: control = 0, case = 1\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting levels: control = 0, case = 1\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting levels: control = 0, case = 1\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting levels: control = 0, case = 1\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this loop is for our purposes, we will not submit it\n",
    "scores = {}\n",
    "for (j in 1:5) {\n",
    "    setwd(paste(\"split_\", j, sep=\"\"))\n",
    "    # below this comment is submitted code unless otherwise stated\n",
    "    train = read.table(\"train.tsv\",\n",
    "                       stringsAsFactors = FALSE,\n",
    "                       header = TRUE)\n",
    "    train$review <- gsub('<.*?>', ' ', train$review)\n",
    "\n",
    "    # Create matrix corresponding to vocab\n",
    "    it_train = itoken(train$review,\n",
    "                        preprocessor = tolower, \n",
    "                        tokenizer = word_tokenizer)\n",
    "    vectorizer = vocab_vectorizer(create_vocabulary(myvocab, \n",
    "                                                      ngram = c(1L, 2L)))\n",
    "    dtm_train = create_dtm(it_train, vectorizer)\n",
    "    set.seed(7568)\n",
    "    fit = glmnet(x = dtm_train, \n",
    "                    y = train$sentiment, \n",
    "                    alpha = 0.05,\n",
    "                    family='binomial')\n",
    "    \n",
    "    #####################################\n",
    "    # Load test data, and \n",
    "    # Compute prediction\n",
    "    #####################################\n",
    "    test <- read.table(\"test.tsv\", stringsAsFactors = FALSE,\n",
    "                    header = TRUE)\n",
    "\n",
    "    test$review <- gsub('<.*?>', ' ', test$review)\n",
    "\n",
    "    # Create matrix corresponding to vocab\n",
    "    it_test = itoken(test$review,\n",
    "                        preprocessor = tolower, \n",
    "                        tokenizer = word_tokenizer)\n",
    "    vectorizer = vocab_vectorizer(create_vocabulary(myvocab, \n",
    "                                                ngram = c(1L, 2L)))\n",
    "    dtm_test = create_dtm(it_test, vectorizer)\n",
    "    \n",
    "    prob = predict(fit, newx=dtm_test, s=0.05)\n",
    "    output = data.frame(id=c(test$id),\n",
    "                        prob=c(prob))\n",
    "    #####################################\n",
    "    # Store your prediction for test data in a data frame\n",
    "    # \"output\": col 1 is test$id\n",
    "    #           col 2 is the predicted probs\n",
    "    #####################################\n",
    "    write.table(output, file = \"mysubmission.txt\", \n",
    "                row.names = FALSE, sep='\\t')\n",
    "    \n",
    "    # Below this is not submitted code\n",
    "    # move \"test_y.tsv\" to this directory\n",
    "    test.y <- read.table(\"test_y.tsv\", header = TRUE)\n",
    "    pred <- read.table(\"mysubmission.txt\", header = TRUE)\n",
    "    pred <- merge(pred, test.y, by=\"id\")\n",
    "    roc_obj <- roc(pred$sentiment, pred$prob)\n",
    "    score = pROC::auc(roc_obj)\n",
    "    scores[j] = score\n",
    "    setwd('../')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf13e182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>0.964057201905281</li><li>0.963563057599206</li><li>0.963464663929034</li><li>0.964065263401769</li><li>0.963399360846504</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.964057201905281\n",
       "\\item 0.963563057599206\n",
       "\\item 0.963464663929034\n",
       "\\item 0.964065263401769\n",
       "\\item 0.963399360846504\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.964057201905281\n",
       "2. 0.963563057599206\n",
       "3. 0.963464663929034\n",
       "4. 0.964065263401769\n",
       "5. 0.963399360846504\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 0.9640572 0.9635631 0.9634647 0.9640653 0.9633994"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac43566",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
